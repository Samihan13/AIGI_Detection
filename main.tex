% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)



\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage{cvpr}      % To produce the REVIEW version


% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Team 8: From Real to Rendered: A Transfer Learning Approach for Detection of AI-Generated Images using ResNet  }

\author{
Samihan Narendra Apte\\
7061559\\
\and
Niranjan Madan\\
7062325\\
\and
Karan Rajshekar\\
7062715\\
}
\maketitle

%%%%%%%%% BODY TEXT
\section{Project Goals and Methodology}

The project explores the efficacy of transfer learning for detecting AI-generated images (AIGI) using a modified ResNet50 architecture. We have integrated custom layers - a Fractal Dimension Layer followed by a fully connected layer to enhance the model's detection capabilities. This approach is distinct from conventional transfer learning due to the unique nature of AIGI detection, which requires capturing intricate and subtle artefacts present in AI-generated images.

The Fractal Dimension Layer calculates the fractal dimension of feature maps, enabling the model to detect self-similar patterns. Such patterns are common in natural images but may differ in AI-generated content \cite{fractal1,fractal2}.  This layer is placed after the initial convolutional layers, allowing it to analyze deeper and more abstract features, enhancing its ability to discern subtle anomalies typical of AIGI.

For this layer, we're using 'Box of Counting' method whose idea is to divide the image into boxes of varying sizes and count how many boxes are needed to cover the image at each scale. Natural images tend to have self-similar patterns in the image at multiple levels. For example, the cropped-in scene of a branch and the sub-branches of a tree has similarities between the zoomed-out scene of the tree and its branches. This characteristic of real images is what the project tries to exploit. 

\section{Progress Report}

\begin{itemize}
    \item Decision on the additional components to be included in the base model: After referring to related works, we have decided to include the following additional layers to enhance the model's performance in detecting AIGIs.
    \begin{itemize}
        \item Fractal Dimension analysis layer
        \item Fully connected layer
    \end{itemize}

    \item Dataset: We have acquired the openly accessible CIFAKE dataset \cite{cifake}. The dataset contains 60,000 synthetically-generated images and 60,000 real images. The real images have been collected from the popular CIFAR-10 dataset. Each image is an RGB image of 24 bits per pixel and has a resolution of 32x32 pixels.

    \item Code completion: We have built the code for the training-from-scratch part as well as the transfer-learning part. We are yet to perform training and evaluation of the models.

    \item Self-supervised method: We are yet to complete building the code for the self-supervised pre-training.
    
\end{itemize}

\section{Problems Encountered}

\subsection{Additional losses to train the specific layers}

We plan to integrate a fractal dimension regularization loss with the classic BCE loss. However, the issue we're facing is with deciding the ideal desired value to compare against. Our initial approach for creating this updated loss was to calculate the fractal dimensions of every "real" image and take an average, but we are uncertain if that is the correct way to go about it since the fractal dimension could vary significantly between the various real images.

\subsection{Dataset for self-supervised method}

For comparison with a self-supervised method, we require a lot of unlabelled data. So far, we plan to strip off the labels of the CIFAKE dataset that we have and combine the real and fake images in the training set. Then we use this combined set of images after applying rotations to train a ResNet50 model to predict what angle the applied rotation is. Then we plan to import these weights to a fresh model, to compare as a self-supervised method. However, we are uncertain if this is the correct method and the right way to proceed, especially regarding preparing the unlabelled data, and the number of images required in it. 

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{references}

\begin{thebibliography}{99}

\bibitem{fractal1} Hirokatsu, K., Asato, M., Eisuke, Y., Ryosuke, Y., Nakamasa, I., Nakamura, A. and Yutaka, S., 2022. Pre-training without natural images. International Journal of Computer Vision, 130(4), pp.990-1007.

\bibitem{fractal2} Husain, A., Nanda, M.N., Chowdary, M.S. and Sajid, M., 2022. Fractals: an eclectic survey, part-I. Fractal and Frac tional, 6(2), p.89.

\bibitem{cifake} Bird, J.J. and Lotfi, A., 2024. Cifake: Image classification and explainable identification of ai-generated synthetic images. IEEE Access.

\end{thebibliography}
}

\end{document}
